{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Imports\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Code Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [\"URL\", \"TITLE\", \"DATE\", \"START_TIME\", \"LOCATION\", \"DESCRIPTION\"]\n",
    "page_amount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_past_events_urls(verbose = False):\n",
    "    pages = list(range(1, page_amount + 1))\n",
    "    url_of_events = []\n",
    "\n",
    "    for page in pages:\n",
    "        clear_output(wait=True) if not(verbose) else None\n",
    "        page_url = \"https://datascience.ucsd.edu/news-and-events/events-2/?page_id_all={}\".format(page)\n",
    "        page_request = requests.get(page_url)\n",
    "        print(\"*** GETTING EVENT URLS *** \")\n",
    "        print(\"Next Page: {}\".format(page_url))\n",
    "        print(\"Status: {}\".format(page_request))\n",
    "\n",
    "        soup = BeautifulSoup(page_request.text, 'html.parser')\n",
    "        events = soup.find_all(\"div\", attrs=\"page-section\")[1].find_all('a', {'href': re.compile(r'https:\\/\\/datascience\\.ucsd\\.edu\\/events')})\n",
    "\n",
    "        for event in events:\n",
    "            event_url = event.get(\"href\")\n",
    "            url_of_events.append(event_url)\n",
    "        print(\"{} URL's Obtained \\n\".format(len(events)))\n",
    "        \n",
    "    return url_of_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_details(url):\n",
    "    page = pd.Series(index=index)\n",
    "    page_request = requests.get(url)\n",
    "    soup = BeautifulSoup(page_request.text, 'html.parser')\n",
    "    \n",
    "    # Add URL to page\n",
    "    page.URL = url\n",
    "    \n",
    "    # Get Title of Event\n",
    "    event_title = soup.find(\"div\", [\"pageinfo\"]).text.strip()\n",
    "    page.TITLE = event_title\n",
    "    \n",
    "    # Get Event Logisitcs\n",
    "    event_logistics = soup.find(\"ul\", [\"post-options\"]).find_all(\"li\")\n",
    "    event_date = event_logistics[0].find(\"span\", [\"cs-event-time\"]).text.strip()\n",
    "    event_location = event_logistics[2].text.strip()\n",
    "\n",
    "    page.DATE = event_date\n",
    "    page.LOCATION = event_location\n",
    "    \n",
    "    event_start_time = re.sub(\"\\xa0to \\xa0.*\", \" \", event_logistics[1].text.strip()).strip()\n",
    "    try:\n",
    "        event_start_time_pd = pd.to_datetime(event_start_time)\n",
    "    except (ValueError):\n",
    "        event_start_time_pd = event_start_time\n",
    "    page[\"START_TIME\"] = event_start_time_pd\n",
    "\n",
    "    \n",
    "    # Get Event Description\n",
    "    event_description_section = soup.find(\"div\", [\"rich_editor_text\"]).find_all([\"p\", \"h1\", \"h2\", \"h3\"])\n",
    "    event_description_formatted = []\n",
    "    for event_description_p in event_description_section:\n",
    "        event_description_formatted.append(event_description_p.text.strip())\n",
    "    event_description = \"\\n\".join(event_description_formatted)\n",
    "    page.DESCRIPTION = re.sub('Event Description\\n', '', event_description).strip()\n",
    "    \n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_list(urls, verbose = False):\n",
    "    urls_processed = 0\n",
    "    df = pd.DataFrame(columns = index)\n",
    "    for url in urls:\n",
    "        event = get_event_details(url)\n",
    "        df = df.append(event, ignore_index=True)\n",
    "        clear_output(wait=True) if not(verbose) else None\n",
    "        urls_processed += 1\n",
    "        print(\"*** GETTING EVENT DESCRIPTIONS ***\")\n",
    "        print(\"{}%\".format(np.round((urls_processed / len(urls)) * 100, 1)))\n",
    "    print(\"*** EVENT DESCRIPTIONS DOWNLOADED  ***\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events(verbose = False, export = True):\n",
    "    past_event_csv_name = str(datetime.now()) + \" past_events.csv\"\n",
    "    print('Today is: ' + str(datetime.now()) + ' UTC')\n",
    "    urls = get_past_events_urls(verbose)\n",
    "    event = events_list(urls, verbose)\n",
    "    clear_output(wait=True) if not(verbose) else None\n",
    "    print(\"*** EXPORTING EVENTS ***\") if export else None\n",
    "    event.to_csv(past_event_csv_name) if export else None\n",
    "    print(\"*** DONE ***\")\n",
    "    return event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_events_page(log_file = False):\n",
    "    '''\n",
    "    Scrapes event page\n",
    "    Parameters:\n",
    "    arg 1 (boolean): If true, system will write log to log file instead of print on notebook\n",
    "    '''\n",
    "    if log_file:\n",
    "        print(\"Beginning Scraping: NOTE LOG OUTPUT WILL BE SENT TO LOG FILE\")\n",
    "        import sys\n",
    "        # keep original sys.stdout\n",
    "        orig_stdout = sys.stdout\n",
    "        \n",
    "        # redirect sys.stdout to log file\n",
    "        with open(\"log.txt\", \"w\") as f:\n",
    "            sys.stdout = f\n",
    "            events = get_events(verbose = True)\n",
    "        \n",
    "        # return to original sys.stdout\n",
    "        sys.stdout = orig_stdout\n",
    "        print(\"Finished! See log file for details\")\n",
    "    else:\n",
    "        # print out log\n",
    "        print(\"Beginning Scraping: NOTE LOG OUTPUT WILL BE PRINTED ON SCREEN\")\n",
    "        events = get_events(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run code below to scrape webpage\n",
    "Make sure to run all above code first though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Scraping: NOTE LOG OUTPUT WILL BE SENT TO LOG FILE\n"
     ]
    }
   ],
   "source": [
    "page_amount = 12 # how many pages there are\n",
    "# set below false to not generate log file and instead print log\n",
    "scrape_events_page(log_file = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
